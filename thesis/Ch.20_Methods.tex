\chapter{Methods\label{methods}}
Three research questions and a hypothesis are defined to find out if there is any significant difference to response times of a server side JavaScript component in different environments.
To answer these questions data is collected from the component and a hybrid literature review is performed.

\section{Research questions}
\begin{itemize}
    \item RQ1\label{RQ1}: What is the response time of a server side monolith JavaScript application?
    \item RQ2\label{RQ2}: What is the response time of a server side independent JavaScript service?
    \item RQ3\label{RQ3}: What is the relation of the response time in different environments?
\end{itemize}

The first research question (RQ1) aims to provide data related to the response time of JavaScript component. The component is running as part of the monolith application.

The second research question (RQ2) aims to provide data related to the response time of JavaScript component. The component is running as an independent service.

The third research question (RQ3) provides statistical analysis based on the data provided by the first (RQ1) and second (RQ2) research questions.

\section{Hypothesis}
In order to test if there is statistical difference between response times in different environments a one sided Z hypothesis test is performed.
Hypothesis test contains null hypothesis \textbf{H0} and alternative hypothesis \textbf{H1}.

\textbf{H0}\label{H0}: $\mu_1\le\mu_2$

\textbf{H1}\label{H1}: $\mu_1>\mu_2$

Where $\mu_1$ is the mean response time of the component as a part of a monolith application and $\mu_2$ is the mean response time of the component in independent service.

\section{Literature}
To find existing literature related to the research a two phase hybrid literature search was performed.
First phase contained a modified systematic literature review with predefined keywords.
The seconds phase used snowballing method with one iteration on the literature found on first phase.
One iteration consisted one back step and one forward step for each final results from phase one.

\subsection{Literature review}
The first search phase is a literature review to find out relevant literature to the case.

\subsubsection{Review protocol}
To find out about related research a five step literature review was implemented.

\begin{enumerate}
    \item Define the review protocol.
    \item Search literature based on the protocol.
    \item Filter results by title and meta-data.
    \item Review findings abstracts by criteria.
    \item Analyze relevant results.
\end{enumerate}

The first step defines the protocol used in the search.

The second step search for the related research based on the protocol.

The third step performs the first analysis of the results to find out relevant research.

The fourth step reviews filtered articles and gives them a quality score.

The fifth steps performs the analysis of the results from the fourth step.

\subsubsection{The search}
The search was performed on the following search engines: 
\begin{itemize}
    \item 1. search engine
    \item ...
\end{itemize}.

The search was performed on the title, abstract and meta-data for the following key words:
\begin{itemize}
    \item word1
    \item word2
    \item ...
\end{itemize}.

For each search engine we used \textbf{SEARCH\_STRING} to find out related research.
Only research from years \textbf{YEARS} was included.

The search returned \textbf{HITS} hits.
\textbf{TABLE\_WITH\_RESULTS\_PER\_ENGINE}


\subsubsection{Initial filtering}
The relevance of the each results was analyzed based on the title and meta-data related to the keywords. Each result was given score based on the point system.

\subsubsection{Quality Review}
Then the quality of found articles from the initial filtering was performed.
Each article was review based on the review criteria and abstract.
Each criteria was scored based on the point system.

For reviewing the relevance of the articles each abstract was reviewed.

\begin{flushleft}
\begin{tabular}{|c c|} 
 \hline
 Criteria & Description \\ [0.5ex] 
 \hline
  Cr.1 & JavaScript performance review  \\ 
  \hline
  Cr.2 & Node.js performance review  \\ 
  \hline
  Cr.3 & Architectural performance review  \\ 
  \hline
  Cr.4 & Response time performance review  \\ 
  \hline
  Cr.5 & Server side performance review  \\ 
  \hline
\end{tabular}
\end{flushleft}


\begin{flushleft}
\begin{tabular}{|c c|} 
 \hline
 Points & Description \\ [0.5ex] 
 \hline
  0 & No relevance to the criteria  \\ 
 \hline
  0.5 & Some relevance to the criteria \\ 
 \hline
 1 & Relevant to the criteria \\ 
 \hline
\end{tabular}
\end{flushleft}

\textbf{A table of results and total points}

\subsubsection{Final result}
For the final results only articles with the relevance point at or above \textbf{relevance point threshold} was included.

The final results with the quality score can be found in the \textbf{TABLE}.

Add tables of results grouped by relevant data, category, relevance score, quality score.

\subsection{Snowballing - Phase two}
The snowballing method was performed on the final results from phase one.
The method included 
\begin{enumerate}
    \item All references from phase one results.
    \item Articles that referred previous steps results.
    \item The initial filtering was performed on the all found result.
    \item Each result were review for their quality.
    \item Final result were analyzed. 
\end{enumerate}

The result can be found in the \textbf{reference table}.

\section{Data}
% What data was collected?
To measure the response time of the component a series of data points were collected.
The data points were collected from two environments.
In first environment the component was integrated as a part of monolith application.
Second environment contained the component and the needed component for communication with the component.

The measured data contains run-time information about the component.
A single data point contains five values.
\begin{enumerate}
    \item Cycle id
    \item Timestamp id
    \item System time (in nanoseconds)
    \item System free memory
    \item System total memory
    \item Used process CPU percentage
\end{enumerate}

The cycle id is unique for each cycle.
The memory and CPU usages were collected to ensure that the system had enough resources to operate.
Data points were collected at the beginning of the application cycle, at the end of the cycle and before and after each third party or internal function call.

For each environment the data was saved at run-time and persisted at the end of each cycle.
Sometimes the cycle was interrupted by an error or the collected data was not complete.
Only valid data was analyzed.
Valid data contains data from full cycle excluding all error cases,

\textbf{Table about how many total data points per environment and how many full cycles}

\textbf{Table containing mean, standard deviation, sample size and standard error for each module, environment and external calls for each environment}




% What data was included and what excluded? And reasons

% What data sources are used and why?


% How data from the component was collected

% How data was analysed

% How the data can answer the research questions

% How case size (data points) was defined


