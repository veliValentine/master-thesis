\chapter{Methods\label{methods}}
Research methods includes defining the research question, literature review and data collection.
One research question is asked in order to answer if there is difference in server side Node.js performance when it is refactored from monolith to independent service.
Literature review is performed to find literature related to research question.
Data collection methods provides insight related to the case and the research question.

\section{Research questions}
In order to see how the performance of Node.js component differs in monolith environment and microservice environment a research question is asked.

\textbf{RQ1}: How does refactoring Node.js component from monolith to microservice affect its performance?

Answering the research question \textbf{RQ1} should give us some insight what the performance of a Node.js component is as a part of a monolith environment and as a part of a microservice environment and is there any performance difference between these environments.

\section{Literature}
Literature review was performed in order to find related literature that provides insight to \textbf{RQ1}.
The review was performed in order to find studies that are related to performance of node.js application, performance changes when refactoring component from monolith to microservice and node.js V8 benchmarks.

The review was performed using review protocol that included four steps.
Review protocol was defined in order to provide reproducible framework for reviewing found literature.
The review protocol is following:
\begin{enumerate}
    \item Define the review protocol. This step defines used search engines, used search string, inclusion and exclusion criteria.
    \item Search literature from defined engines using defined search string.
    \item Review results from previous step according to inclusion criteria.
    \item Review results from previous step according to exclusion criteria.
\end{enumerate}

In order to find literature that gives insight to \textbf{RQ1} a literature search was performed.
Four different search engines were selected to be used for finding related literature.
Used engines can be seen in table \ref{table:literature:searchEngines}.

\begin{table}[ht!]
    \begin{tabular}{| c | c |} 
        \hline
        Engine
        & Url
        \\ [0.5ex] \hline
        
        Helka (University of Helsinki library)
        & \url{https://helka.helsinki.fi/}
        \\ \hline
        
        IEEE Xplore
        & \url{https://ieeexplore.ieee.org}
        \\ \hline
        
        Google Scholar
        & \url{https://scholar.google.com/}
        \\ \hline

        Scopus
        & \url{https://www.scopus.com/home.uri}
        \\ \hline
    \end{tabular}    
    \caption{Used search engines.}
    \label{table:literature:searchEngines}
\end{table}

Search was performed using search string that matches literature title only.
The search string was modified using documentation of used search engine allowing results not to be limited for any spelling differences.
For example search including "microservice" was modified so that "micro-service" and similar differences were included in the results.
Search string used before modifications was "\textit{performance AND (V8 OR JavaScript OR Node.js OR (microservice AND monolith))}".

Results from search were reviewed based on inclusion criteria seen on table \ref{table:literature:inclusionCriteria}.
Each results title and abstracts were review according to inclusion criterion.
Each criterion were given zero, one or two points based on their relevance to the criterion.
Zero points were given to results that has no relevance to the criterion, one point to results that has some relevance to criterion and two points to results that are relevant to the criterion.
The points system can be seen in table \ref{table:literature:pointSystem}.
Inclusion score for a result was the sum of points from each criterion.
Each results could receive minimum of $0$ points and maximum of $14$ points.

\begin{table}[ht!]
    \begin{tabular}{|c c|} 
        \hline
        Criteria & Description \\ [0.5ex] 
        \hline
        Cr.1 & JavaScript performance review
        \\ 
        \hline
        Cr.2 & Node.js performance review  \\ 
        \hline
        Cr.3 & Server side performance review  \\ 
        \hline
        Cr.4 & V8 engine performance review  \\ 
        \hline
        Cr.5 & Monolith performance review  \\ 
        \hline
        Cr.6 & Microservice performance review  \\ 
        \hline
        Cr.7 & Response time performance review  \\ 
        \hline
    \end{tabular}    
    \caption{Inclusion Criteria.}
    \label{table:literature:inclusionCriteria}
\end{table}

\begin{table}[ht!]
    \begin{tabular}{|c c|}
        \hline
        Description & Points \\ [0.5ex] 
        \hline
         No relevance to the criterion & 0  \\ 
        \hline
        Some relevance to the criterion & 1 \\ 
        \hline
        Relevant to the criterion & 2 \\ 
        \hline
    \end{tabular}
    \caption{Point system for inclusion criterion.}
    \label{table:literature:pointSystem}
\end{table}

Exclusion of results were based on exclusion criteria seen in table \ref{table:literature:exclusionCriteria}.
If the result matched any exclusion criterion it was excluded from the final result.
Exclusion criteria removes all literature not relevant to the \textbf{RQ1}.
Literature that were not available, i.e. not available for university of Helsinki student, nor in English were also excluded.

\begin{table}[ht!]
    \begin{tabular}{|c c|} 
        \hline
        Criteria & Description \\ [0.5ex] 
        \hline
        Cr.1 & Inclusion score less than 3.5 \\
        \hline
        Cr.2 & Only client side performance  \\
        \hline
        Cr.3 & Not available \\
        \hline
        Cr.4 & Not in English \\
        \hline
    \end{tabular}    
    \caption{Exclusion Criteria}
    \label{table:literature:exclusionCriteria}
\end{table}

Final results contained 16 out of 146 related literature.
Detailed results and review is found in the results chapter.

\section{Collecting Sample Data}
To measure the performance of the component a series of data points were collected along each cycle.
The performance data were collected from both environments sequentially over two month period.
The environments were running in production with real data.
In the first environment the component was integrated as a part of monolith application.
In the second environment the component was refactored into independent service containing only the studied component and the needed functionality for communication with the component.

To measured performance of the component a sample of data points were collected at run-time.
Each data point contains the cycle identification number, the run-time environment, system timestamp and information from system hardware.
A single data point contains all these values.
\textbf{TODO describe in more detail}
\begin{enumerate}
    \item Cycle id
    \item Run-time environment id
    \item System time (in nanoseconds)
    \item System free memory (in bytes)
    \item System total memory (in bytes)
    \item Used CPU percentage (used CPU over total usable CPU)
\end{enumerate}

The cycle id is unique for each cycle and environment.
The system time and run-time environment were collected to collect data required to measure the response time.
The memory and CPU usages were collected to ensure that the system had enough resources to operate.
Data were collected along each cycle.
This included the start and end of the cycle, and before and after each external data call.
The data was saved at run-time and persisted at the end of each cycle.
Sometimes the cycle was interrupted by an error or the collected data was not complete resulting partial data to be persisted.
