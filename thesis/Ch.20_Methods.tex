\chapter{Methods\label{methods}}
Research methods includes defining the research question, literature review and data collection.
One research question is asked in order to answer if there is difference in server side Node.js performance when it is refactored from monolith to independent service.
Literature review is performed to find literature related to research question.
Data collection methods provides insight related to the case and the research question.

In order to see how the performance of Node.js component differs in monolith environment and microservice environment a research question is asked.

\textbf{RQ1}: How does refactoring Node.js component from monolith to microservice affect its performance?

Answering the research question \textbf{RQ1} should give us some insight what the performance of a Node.js component is as a part of a monolith environment and as a part of a microservice environment and is there any performance difference between these environments.

\section{Literature Review}
Literature review was performed in order to find related literature that provides insight to \textbf{RQ1}.
The review was performed in order to find studies that are related to performance of node.js application, performance changes when refactoring component from monolith to microservice and node.js V8 benchmarks.

The review was performed using review protocol that included four steps.
Review protocol was defined in order to provide reproducible framework for reviewing found literature.
The review protocol is following:
\begin{enumerate}
    \item Define the review protocol. This step defines used search engines, used search string, inclusion and exclusion criteria.
    \item Search literature following the review protocol.
    \item Include found literature following the review protocol.
    \item Exclude found literature following the review protocol.
\end{enumerate}

Four different search engines were selected to be used for finding related literature.
Used engines can be seen in table \ref{table:literature:searchEngines}.
\begin{table}[ht!]
    \begin{tabular}{| c | c |} 
        \hline
        Engine
        & Url
        \\ [0.5ex] \hline
        
        Helka (University of Helsinki library)
        & \url{https://helka.helsinki.fi/}
        \\ \hline
        
        IEEE Xplore
        & \url{https://ieeexplore.ieee.org}
        \\ \hline
        
        Google Scholar
        & \url{https://scholar.google.com/}
        \\ \hline

        Scopus
        & \url{https://www.scopus.com/home.uri}
        \\ \hline
    \end{tabular}    
    \caption{Used search engines.}
    \label{table:literature:searchEngines}
\end{table}
Search was performed using search string that matches literature title only.
The search string was modified using documentation of used search engine allowing results not to be limited for any spelling differences.
For example search including "microservice" was modified so that "micro-service" and similar differences were included in the results.
Search string used without modifications was "\textit{performance AND (V8 OR JavaScript OR Node.js OR (microservice AND monolith))}".

Results from search were reviewed based on inclusion criteria seen on table \ref{table:literature:inclusionCriteria}.
Each results title and abstracts were review according to inclusion criterion.
Each criterion were given zero, one or two points based on their relevance to the criterion.
Zero points were given to results that has no relevance to the criterion, one point to results that has some relevance to criterion and two points to results that are relevant to the criterion.
The points system can be seen in table \ref{table:literature:pointSystem}.
Inclusion score was the sum of points from each criterion with minimum inclusion score of $0$ points and maximum of $14$ points.

\begin{table}[ht!]
    \begin{tabular}{|c c|} 
        \hline
        Criteria & Description \\ [0.5ex] 
        \hline
        Cr.1 & JavaScript performance review
        \\ 
        \hline
        Cr.2 & Node.js performance review  \\ 
        \hline
        Cr.3 & Server side performance review  \\ 
        \hline
        Cr.4 & V8 engine performance review  \\ 
        \hline
        Cr.5 & Monolith performance review  \\ 
        \hline
        Cr.6 & Microservice performance review  \\ 
        \hline
        Cr.7 & Response time performance review  \\ 
        \hline
    \end{tabular}    
    \caption{Inclusion Criteria.}
    \label{table:literature:inclusionCriteria}
\end{table}

\begin{table}[ht!]
    \begin{tabular}{|c c|}
        \hline
        Description & Points \\ [0.5ex] 
        \hline
         No relevance to the criterion & 0  \\ 
        \hline
        Some relevance to the criterion & 1 \\ 
        \hline
        Relevant to the criterion & 2 \\ 
        \hline
    \end{tabular}
    \caption{Point system for inclusion criterion.}
    \label{table:literature:pointSystem}
\end{table}

Exclusion of results were based on exclusion criteria seen in table \ref{table:literature:exclusionCriteria}.
If the result matched any exclusion criterion it was excluded from the final result.
Exclusion criteria removes all literature not relevant to the \textbf{RQ1}.
Literature that were not available, i.e. not available for university of Helsinki student, nor in English were also excluded.

\begin{table}[ht!]
    \begin{tabular}{|c c|} 
        \hline
        Criteria & Description \\ [0.5ex] 
        \hline
        Cr.1 & Inclusion score less than 3.5 \\
        \hline
        Cr.2 & Only client side performance  \\
        \hline
        Cr.3 & Not available \\
        \hline
        Cr.4 & Not in English \\
        \hline
    \end{tabular}    
    \caption{Exclusion Criteria}
    \label{table:literature:exclusionCriteria}
\end{table}

Final results contained 16 out of 146 related literature.
Detailed results and review can be found in the results chapter.

\section{Collecting Sample Data}
To measure the performance of the component a series of data points were collected along all cycles.
The data was collected sequentially over two month period first from independent service and then from monolith.
The components were using real production data so it was important that the environments won't affect each other during runtime.

To measured performance of the component a sample of data points were collected at run time.
Single datap point contained:
\begin{enumerate}
    \item Cycle id. A way to identify and group collected data to single cycle.
    \item Run-time environment id. An id to separate the collected data between the used environments.
    \item System time (in nanoseconds). Systems timestamp in nanoseconds at the collected time.
    \item System free memory (in bytes). The available free memory that the application is able to consume at the collected time.
    \item System total memory (in bytes). The total memory that the used hardware is able to provide at the collected time.
    \item Used CPU percentage. Ratio of the used CPU power over the total available CPU power at the collected time.
\end{enumerate}

The cycle id is unique for each cycle and environment.
The system time and run-time environment were collected to collect data required to measure the response time.
The memory and CPU usages were collected to ensure that the system had enough resources to operate.
Data were collected along each cycle.
This included the start and end of the cycle, and before and after each external data call.
The data was saved at run-time and persisted at the end of each cycle.
Sometimes the cycle was interrupted by an error or the collected data was not complete resulting partial data to be persisted.
