\chapter{Methods\label{methods}}
A research question is defined to find out if there is any significant difference in performance of a server side JavaScript component in different environments.
To answer these questions performance samples are collected from the component and their significance evaluated.

\section{Research questions}
In order to see how the performance of Node.js component differs in monolith and microservice environment a research question is defined.

\textbf{RQ1}: How does refactoring a server side node.js component from monolith to a part of microservice affect the performance of the component?

Answering the research question \textbf{RQ1} should give us some insight what the performance of a Node.js component is as a part of a monolith environment and as a part of a microservice environment and how these performances relate to each other.

\section{Hypothesis}
In order to see if there is statistical significance between response times in different environments a one sided significance test is performed.
The test contains null hypothesis \textbf{H0} $\mu_m\le\mu_i$ and alternative hypothesis \textbf{H1} $\mu_m>\mu_i$ where $\mu_m$ is the mean response time of the component as a part of a monolith application and $\mu_i$ is the mean response time of the component in independent service.

\section{Literature}
To find existing literature related to the research a two phase hybrid literature search was performed.
First phase contained a modified systematic literature review with predefined keywords.
The seconds phase used snowballing method with one iteration on the literature found on first phase.
One iteration consisted one back step and one forward step for each final results from phase one.

\subsection{Literature review}
The first search phase is a literature review to find out relevant literature to the case.

\subsubsection{Review protocol}
To find out about related research a five step literature review was implemented.

\begin{enumerate}
    \item Define the review protocol.
    \item Search literature based on the protocol.
    \item Filter results by title and meta-data.
    \item Review findings abstracts by criteria.
    \item Analyze relevant results.
\end{enumerate}

The first step defines the protocol used in the search.

The second step search for the related research based on the protocol.

The third step performs the first analysis of the results to find out relevant research.

The fourth step reviews filtered articles and gives them a quality score.

The fifth steps performs the analysis of the results from the fourth step.

\subsubsection{The search}
The search was performed on the following search engines: 
\begin{itemize}
    \item 1. search engine
    \item ...
\end{itemize}.

The search was performed on the title, abstract and meta-data for the following key words:
\begin{itemize}
    \item word1
    \item word2
    \item ...
\end{itemize}.

For each search engine we used \textbf{SEARCH\_STRING} to find out related research.
Only research from years \textbf{YEARS} was included.

The search returned \textbf{HITS} hits.
\textbf{TABLE\_WITH\_RESULTS\_PER\_ENGINE}


\subsubsection{Initial filtering}
The relevance of the each results was analyzed based on the title and meta-data related to the keywords. Each result was given score based on the point system.

\subsubsection{Quality Review}
Then the quality of found articles from the initial filtering was performed.
Each article was review based on the review criteria and abstract.
Each criteria was scored based on the point system.

For reviewing the relevance of the articles each abstract was reviewed.

\begin{flushleft}
\begin{tabular}{|c c|} 
 \hline
 Criteria & Description \\ [0.5ex] 
 \hline
  Cr.1 & JavaScript performance review  \\ 
  \hline
  Cr.2 & Node.js performance review  \\ 
  \hline
  Cr.3 & Architectural performance review  \\ 
  \hline
  Cr.4 & Response time performance review  \\ 
  \hline
  Cr.5 & Server side performance review  \\ 
  \hline
\end{tabular}
\end{flushleft}


\begin{flushleft}
\begin{tabular}{|c c|} 
 \hline
 Points & Description \\ [0.5ex] 
 \hline
  0 & No relevance to the criteria  \\ 
 \hline
  0.5 & Some relevance to the criteria \\ 
 \hline
 1 & Relevant to the criteria \\ 
 \hline
\end{tabular}
\end{flushleft}

\textbf{A table of results and total points}

\subsubsection{Final result}
For the final results only articles with the relevance point at or above \textbf{relevance point threshold} was included.

The final results with the quality score can be found in the \textbf{TABLE}.

Add tables of results grouped by relevant data, category, relevance score, quality score.

\subsection{Snowballing - Phase two}
The snowballing method was performed on the final results from phase one.
The method included 
\begin{enumerate}
    \item All references from phase one results.
    \item Articles that referred previous steps results.
    \item The initial filtering was performed on the all found result.
    \item Each result were review for their quality.
    \item Final result were analyzed. 
\end{enumerate}

The result can be found in the \textbf{reference table}.

\section{Collecting Sample Data}
To measure the performance of the component a series of data points were collected along each cycle.
The performance data were collected from both environments over two month period.
In the first environment the component was integrated as a part of monolith application.
In the second environment the component was refactored into independent service containing only the studied component and the needed functionality for communication with the component.

To measured performance of the component a sample of data points were collected at run-time.
Each data point contains the cycle identification number, the run-time environment, system timestamp and information from system hardware.
A single data point contains all these values.
\begin{enumerate}
    \item Cycle id
    \item Run-time environment id
    \item System time (in nanoseconds)
    \item System free memory (in bytes)
    \item System total memory (in bytes)
    \item Used CPU percentage (used CPU over total usable CPU)
\end{enumerate}

The cycle id is unique for each cycle and environment.
The system time and run-time environment were collected to collect data required to measure the response time.
The memory and CPU usages were collected to ensure that the system had enough resources to operate.
Data were collected along each cycle.
This included the start and end of the cycle, and before and after each external data call.
The data was saved at run-time and persisted at the end of each cycle.
Sometimes the cycle was interrupted by an error or the collected data was not complete leaving partial data in the sample.
