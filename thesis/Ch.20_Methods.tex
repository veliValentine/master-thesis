\chapter{Methods\label{methods}}
Research methods includes defining the research question, literature review and data collection.
One research question is asked in order to answer if there is difference in server side Node.js performance when it is refactored from monolith to independent service.
Literature review is performed to find literature related to research question.
Data collection methods provides insight related to the case and the research question.

\section{Research questions}
In order to see how the performance of Node.js component differs in monolith environment and microservice environment a research question is asked.

\textbf{RQ1}: How does refactoring Node.js component from monolith to microservice affect its performance?

Answering the research question \textbf{RQ1} should give us some insight what the performance of a Node.js component is as a part of a monolith environment and as a part of a microservice environment and is there any performance difference between these environments.


\section{Literature}
Literature review was performed in order to find related literature that provides insight to \textbf{RQ1}.

\subsection{Literature Review}
Literature review was performed in order to find studies that are related to performance of Node.js application, performance changes when refactoring component from monolith to microservice and Node.js V8 benchmarks.

The review was performed using review protocol that included four steps.
Review protocol was defined in order to provide reproducible framework for reviewing found literature.
The review protocol is following:
\begin{enumerate}
    \item Define the review protocol
    \item Search literature based on the protocol.
    \item Review literature based on inclusion criteria
    \item Review literature based on exclusion criteria
\end{enumerate}

The first step defines the used protocol.
This includes defining search engines, used search string, inclusion criteria and exclusion criteria.
The second step performs the search for the related literature based on the defined protocol.
Third step decides the results that are included and fourth step the results that are excluded.
% 1. Search string, used engines with twists. Included articles related to title.
\subsubsection{Protocol}
In order to find literature that gives insight to \textbf{RQ1} a literature search was performed.
Four different search engines were used for finding related literature.
Used engines can be seen in table \ref{table:literature:searchEngines}.

\begin{table}[h!]
    \begin{tabular}{| c | c |} 
        \hline
        Engine
        & Url
        \\ [0.5ex] \hline
        
        Helka (University of Helsinki library)
        & \url{https://helka.helsinki.fi/}
        \\ \hline
        
        IEEE Xplore
        & \url{https://ieeexplore.ieee.org}
        \\ \hline
        
        Google Scholar
        & \url{https://scholar.google.com/}
        \\ \hline

        Scopus
        & \url{https://www.scopus.com/home.uri}
        \\ \hline
    \end{tabular}    
    \caption{Used search engines.}
    \label{table:literature:searchEngines}
\end{table}

Search was performed using search string that matches literature title only.
The search string was modified using documentation of used search engine allowing results not to be limited for any spelling differences.
For example search including "microservice" was modified so that "micro-service" and similar differences were included in the results.
Search string used before modifications was "\textit{performance AND (V8 OR JavaScript OR Node.js OR (microservice AND monolith))}".

Results from search were reviewed based on inclusion criteria seen on table \ref{table:literature:inclusionCriteria}.
Each criteria was review based on the results title and abstract.
Each criterion were given zero, one or two points based on their relevance to the criteria.
Zero points were given to results that has no relevance to the criteria, one point to results that has some relevance to criteria and two points to results that are relevant to the criteria.
The points system can be seen in table \ref{table:literature:pointSystem}.
Inclusion score of a result was the sum of points from each criteria
Each results could receive minimum of $0$ points and maximum of $14$ points.

\begin{table}[h!]
    \begin{tabular}{|c c|} 
        \hline
        Criteria & Description \\ [0.5ex] 
        \hline
        Cr.1 & JavaScript performance review
        \\ 
        \hline
        Cr.2 & Node.js performance review  \\ 
        \hline
        Cr.3 & Server side performance review  \\ 
        \hline
        Cr.4 & V8 engine performance review  \\ 
        \hline
        Cr.5 & Monolith performance review  \\ 
        \hline
        Cr.6 & Microservice performance review  \\ 
        \hline
        Cr.7 & Response time performance review  \\ 
        \hline
    \end{tabular}    
    \caption{Inclusion Criteria.}
    \label{table:literature:inclusionCriteria}
\end{table}

\begin{table}[h!]
    \begin{tabular}{|c c|}
        \hline
        Points & Description \\ [0.5ex] 
        \hline
         0 & No relevance to the criteria  \\ 
        \hline
         1 & Some relevance to the criteria \\ 
        \hline
        2 & Relevant to the criteria \\ 
        \hline
    \end{tabular}
    \caption{Point system for inclusion criteria.}
    \label{table:literature:pointSystem}
\end{table}

Exclusion of results were based on exclusion criteria seen in table \ref{table:literature:exclusionCriteria}.
If the result matched any exclusion criteria it was excluded from the final result.
Exclusion criteria removes all literature not relevant to the \textbf{RQ1}.

\begin{table}[h!]
    \begin{tabular}{|c c|} 
        \hline
        Criteria & Description \\ [0.5ex] 
        \hline
        Cr.1 & Inclusion score less than 2.5 \\
        \hline
        Cr.2 & Only client side performance  \\
        \hline
    \end{tabular}    
    \caption{Exclusion Criteria}
    \label{table:literature:exclusionCriteria}
\end{table}

\section{Collecting Sample Data}
To measure the performance of the component a series of data points were collected along each cycle.
The performance data were collected from both environments over two month period.
In the first environment the component was integrated as a part of monolith application.
In the second environment the component was refactored into independent service containing only the studied component and the needed functionality for communication with the component.

To measured performance of the component a sample of data points were collected at run-time.
Each data point contains the cycle identification number, the run-time environment, system timestamp and information from system hardware.
A single data point contains all these values.
\begin{enumerate}
    \item Cycle id
    \item Run-time environment id
    \item System time (in nanoseconds)
    \item System free memory (in bytes)
    \item System total memory (in bytes)
    \item Used CPU percentage (used CPU over total usable CPU)
\end{enumerate}

The cycle id is unique for each cycle and environment.
The system time and run-time environment were collected to collect data required to measure the response time.
The memory and CPU usages were collected to ensure that the system had enough resources to operate.
Data were collected along each cycle.
This included the start and end of the cycle, and before and after each external data call.
The data was saved at run-time and persisted at the end of each cycle.
Sometimes the cycle was interrupted by an error or the collected data was not complete resulting partial data to be persisted.
