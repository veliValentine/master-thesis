\chapter{Results\label{results}}
We are going to analyze the collected performance data and see if there is statistically meaningful difference between the components response time in different environments.

\section{Sample Data}
Performance samples were collected from the component in both environments over two month period.
In order to capture the component performance only full cycles without errors were included in the analysis.
Partial cycles and cycles with errors were excluded from the samples since the performance in these cases was irrelevant to the system.
Hardware samples are shown in tables \ref{table:hardware results:monolith:1} and \ref{table:hardware results:independent service:1}.
The response time samples are shown in tables \ref{table:response time results:1}, \ref{table:response time results:2} and \ref{table:response time results:3}.

\subsubsection{Hardware Samples}
Hardware samples were collected along each cycle to make sure that the components response time was not limited by the used hardware.
Only samples from full cycles without errors were included.
The results for monolith are shown in table \ref{table:hardware results:monolith:1} and for independent service in table \ref{table:hardware results:independent service:1}.
In the monolith environment $206965$ hardware samples was collected from $1649$ full cycles.
From the independent service a total of $26902$ hardware samples was collected from $205$ full cycles.
The hardware samples were collected along response time timestamps.
Hardware samples only reflects the CPU utilization and the system memory status at the point of the timestamp collection and not the status during any meaningful process in the component.

\begin{table}[h!]
    \begin{tabular}{|c|c|c|c|} 
        \hline
        Monolith
        & CPU (\%)
        & Free Memory (Gigabytes)
        & Total Memory (Gigabytes) \\ [0.5ex] 
        
        \hline\hline
        Mean
        & 25.0
        & 7.588166...
        & 64.265863... \\ 
        
        Median
        & 22.0
        & 3.699269...
        & 64.265863... \\ 

        Minimum
        & 6.0
        & 0.391187...
        & 64.265863... \\ 
        
        Maximum
        & 36.0
        & 55.782023...
        & 64.265863... \\
        \hline
    \end{tabular}
    \caption{Hardware Utilization - Monolith $n=206 965$}
    \label{table:hardware results:monolith:1}
\end{table}

Collected hardware samples shows the systems CPU utilization percentage.
The CPU utilization percentage was calculated as
\[
\textit{Utilization percentage} = 100 \cdot \frac{\textit{Available CPU}}{\textit{Total CPU}}
.\]
Where the available CPU is the number of milliseconds the system has spend in the user mode.
The total CPU is the sum of milliseconds the system has spend on user, system and idle modes. The CPU utilization is shown in tables \ref{table:hardware results:monolith:1} and \ref{table:hardware results:independent service:1}.

Collected samples shows that the CPU utilization percentage didn't have any noted effect on the performance since the maximum CPU utilization was at $36.0\%$ in the monolith environment and $1.0\%$ in the independent service.
Leaving most of the CPU available to the application process.
The mean CPU utilization in the monolith is 24 percentage points higher compared to the mean CPU utilization in the independent service.

Collected hardware samples contained information about the free memory and the total available memory for the system.
In table \ref{table:hardware results:monolith:1} the memory data is shown for the monolith application and in table \ref{table:hardware results:independent service:1} for the independent service.
The minimum available memory for the system in monolith was around $0.609...\%$ and in independent service around $61.233...\%$.
These benchmarks was calculated using the following formula
\[
\textit{Minimum available memory} = 100 \cdot \frac{\textit{Minimum free memory}}{\textit{Maximum total memory}}
.\]
It is possible that the system would run out of memory.
Out of memory cycles are not shown in samples since the system would raise \textit{out of memory} exception.
The collected memory samples doesn't show out of memory errors since error cases were excluded from the analysis.
The mean available memory was around $11.807...\%$ in monolith and $66.121...\%$ in independent service leaving plenty of memory available for the application use in the collected samples.

\subsubsection{Response Time Samples}
Timestamp samples were collected from both environments to measure the response time of the component.
The results are shown in tables \ref{table:response time results:1}, \ref{table:response time results:2} and \ref{table:response time results:3}.
Start and end timestamps of each cycle were collected along the timestamps before and after all external calls made from the component.
The response time for a cycle was calculated with the following formula:
\begin{gather*}
\textit{ResponseTime}_n
\\ =\textit{cycleResponseTime}_n - \textit{externalCallResponseTimes}_n
\\ = \textit{cycleEndTimestamp}_n - \textit{cycleStartTimestamp}_n - 
    \sum_{i=1}^{\textit{m}} (\textit{afterExternalCall}_{ni} -     \textit{beforeExternalCall}_{ni}),
\end{gather*}
where $n$ is the nth cycle from samples and $m$ is the made external calls for the cycle.
The timestamps were persisted after each cycle.

Collected samples contained $1649$ full cycles from monolith environment with mean response time of $\sim198.7$ms.
The mean value was $\sim2.48$ times slower against the minimum value of $\sim 80.2$ms and $\sim5.94$ times faster when compared to the maximum value of $\sim1180.6$ms in the monolith environment.

In the independent service a sample of $205$ full cycles were collected with a mean value of $\sim106.2$ms.
The mean value was $\sim3.14$ times slower compared to the minimum value of $\sim 33.76$ms and $\sim3.80$ times faster compared to the maximum value of $\sim 403.94$ms.

Mean response time in monolith was $\sim1.87$ times slower when compared to the mean response time in the independent service.
The minimum value in the monolith was $\sim 66.63$ percentage points greater that the minimum value in the independent service and the maximum value in the monolith was $\sim 213.57$ percentage points greater when compared to the maximum value in the independent service.

\begin{table}[h!]
       \begin{tabular}{|c|c|c|c|} 
        \hline
        Independent Service
        & CPU (\%)
        & Free Memory (Gigabytes)
        & Total Memory (Gigabytes) \\ [0.5ex] 
        
        \hline\hline
        Mean
        & 1.0
        & 2.715358... 
        & 4.106678...
        \\
        
        Median
        & 1.0
        & 2.631246...
        & 4.133356...
        \\ 

        Minimum
        & 1.0
        & 2.530976...
        & 4.072448...
        \\ 
        
        Maximum
        & 1.0
        & 3.034747...
        & 4.133356...
        \\
        \hline
    \end{tabular}
    \caption{Hardware Utilization - Independent Service $n=26902$}
    \label{table:hardware results:independent service:1}
\end{table}

\begin{table}[h!]
    \begin{tabular}{|c|c|c|c|} 
        \hline
        Environment
        & Count (n)
        & Mean (ms)
        & Median (ms)
        \\ [0.5ex] 
        
        \hline\hline
        Monolith
        & 1649
        & 198.747024...
        & 184.522865...
        \\ 
        
        Independent Service
        & 205
        & 106.166740...
        & 88.404872
        \\
        \hline
    \end{tabular}
    \caption{Results for response time samples. Timestamps are in milliseconds.}
    \label{table:response time results:1}
\end{table}

\begin{table}[h!]
    \begin{tabular}{|c|c|c|} 
        \hline
        Environment
        & Min (ms)
        & Max (ms) \\ [0.5ex] 
        
        \hline\hline
        Monolith
        & 80.201486... 
        & 1180.642508...
        \\ 
        
        Independent Service
        & 33.763860
        & 403.937152
        \\
        \hline
    \end{tabular}
    \caption{Results for response time samples. Timestamps are in milliseconds.}
    \label{table:response time results:2}
\end{table}

\begin{table}[h!]
    \begin{tabular}{|c|c|c|} 
        \hline
        Environment
        & Standard Deviation (ns)
        & Standard Error (ns) \\ [0.5ex] 
        
        \hline\hline
        Monolith
        & $85.409388... \cdot 10^6$
        & $2.103271... \cdot 10^6$
        \\ 
        
        Independent Service
        & $77.391723... \cdot 10^6$
        & $5.405272... \cdot 10^6$
        \\ 
         \hline
    \end{tabular}
    \caption{Results for response time samples. Timestamps are in nanoseconds.}
    \label{table:response time results:3}
\end{table}

\begin{table}[h!]
    \begin{tabular}{|c|c|c|} 
        \hline
        \textbf{To Do}
        & Show mean, min and max values from both environments next to each other.
        \\
        \hline
        \textbf{To Do}
        & Show mean value with 95th-percentile confidence interval.
        \\ [0.5ex] 
        \hline
    \end{tabular}
    \caption{Bar graph for samples. 95th-percentile confidence intervals}
    \label{grap:response time results:bar-95th}
\end{table}

\subsubsection{Summary}
The component performance was not limited by the hardware according to the collected hardware sample.
Most of the systems CPU was unused and average of at least $11.8\%$ free memory was available for the systems to use.

The response time being $\sim 1.87$ times faster in independent service when compared to monolith environment.
Overall it seems that the components performance in independent service is better when compared to the component performance in monolith.
To see if there is statistically relevant difference in the components response time a significance test was performed.

\subsection{Significance Tests}
To determine if the response time in monolith environment is statistically slower than the response time in single service a two sample significance test was performed.

Let $\mu_m$ be the mean response time of the component in the monolith and $\mu_i$ be the mean response time of the component in independent service.
To see if the data is sufficient enough to prove that $\mu_m > \mu_i$ a one sided Z test is performed with null hypothesis \textbf{H0}: $\mu_m \leq \mu_i$ against the alternative hypothesis \textbf{H1}: $\mu_m > \mu_i$.

When $\overline{x}_m$ is the sample mean, $S_m^2$ the sample variance and $n_m$ the sample size in the monolith 
and $\overline{x}_i$ the sample mean, $S_i^2$ sample variance and $n_i$ the sample size in the independent service.
Since both samples are large enough a one sided Z test is performed.
The value of test statistic $TS$ is
\[
TS=\frac{\overline{x}_m-\overline{x}_i}{\sqrt{\frac{S_m^2}{n_m}+\frac{S_i^2}{n_i}}}
\]
\[
TS=15.961947... \approx 15.962
.\]
 
With one sided Z test the null hypothesis \textbf{H0} will be rejected when the $p$ value 
equals the probability that a standard normal is as large as $TS$. Calculating the $p$ value we get % It doesn't  make any sense
\[
\text{p value} = P\{Z\geq TS\} = P\{Z\geq 15.962...\} \leq P\{Z\geq 3.49\} = 0.0002
.\]

The $p$ value being at least $0.0002$ the null hypothesis \textbf{H0} can be rejected with any significance level above $0.02\%$.
With significance levels 1\%, 5\% and 10\% the null hypothesis \textbf{H0} was rejected in favor of the alternative hypothesis \textbf{H1}.
The mean response time of the component in the monolith is statistically slower than the mean response time of the component in the independent service.

\section{Literature}
Literature review results.

\subsubsection{Literature Search}
The search was based on the search engines defined in literature review protocol.
The engines can be viewed in table \textbf{ToDo ref to table}.

The search returned \textbf{HITS} hits.
Search results are shown in table \ref{table:literature:searchResults}.

\begin{table}[h!]
    \begin{tabular}{|c|c|c|c|} 
        \hline
        Search Engine
        & Search Hits
        & Unique Hits
        & Dublicate Hits
        \\ 
        \hline
    \end{tabular}    
    \caption{Literature Search Results}
    \label{table:literature:searchResults}
\end{table}

\subsubsection{Included Literature}
\textit{Text containing included literature and results per engine}.
Results with inclusion score can be found in table \ref{table:literature:inclusionResults}

\begin{table}[h!]
    \begin{tabular}{| c | c |}
        \hline
        Title
        & Relevance Score
        \\
        \hline
    \end{tabular}
    \caption{Results with given relevance score}
    \label{table:literature:inclusionResults}
\end{table}

\subsubsection{Excluded Literature}
\textit{Text that shows the result after reviewing results from exclusion}


\subsubsection{Final result}
\textit{Summary about the results}
\textit{Tables about the results - Articles grouped by related category and criteria}