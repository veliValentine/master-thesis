\chapter{Results\label{results}}
The performed literature review results and the collected data analysis is described in this chapter.

\section{Literature Review}
Literature review was performed to find related literature to \textbf{RQ1}.
Review used four search engines that provided 147 initial results when using defined search string.
From the 147 initial search results 93 were unique.
\begin{table}[ht!]
    \begin{tabular}{|c c|} 
        \hline
        Search Engine
        & Search Hits
        \\ 
        \hline\hline
        Helka (University of Helsinki library)
        & 46
        \\ 
        
        IEEE Xplore
        & 26
        \\ 
        
        Google Scholar
        & 7
        \\ 
        
        Scopus
        & 68
        \\ 
        \hline
        Total
        & 147
        \\ 
        \hline
    \end{tabular}    
    \caption{Initial search results of related literature}
    \label{table:literature:initialSearchResults}
\end{table}
Search results title and abstract was reviewed against inclusion criteria.
Highest score received was 8 out of 14 for three unique results and lowest score 0 out of 14 for 24 unique results.
Three results didn't have an abstract to review so they were excluded from the review at this point.
Table \ref{table:literature:inclusionResults} shows inclusion score among initial search results.
\begin{table}[ht!]
    \begin{tabular}{|c c c c|} 
        \hline
        Inclusion Score
        & Count (n)
        & Unique Count (n)
        & Included Literature Id
        \\ 
        \hline\hline
        Score 8/14
        & 7
        & 3
        & 1, 2, 3
        \\ 
        
        Score 7/14
        & 4
        & 2
        & All excluded
        \\ 
        
        Score 6/14
        & 4
        & 4
        & 4
        \\ 
        
        Score 5/14
        & 16
        & 8
        & 5, 6, 7, 8
        \\ 
        
        Score 4/14
        & 17
        & 11
        & 9, 10, 11, 12
        \\ 
        
        Score 3/14
        & 23
        & 14
        & All excluded
        \\ 
        
        Score 2/14
        & 36
        & 20
        & All excluded
        \\ 
        
        Score 1/14
        & 9
        & 4
        & All excluded
        \\ 
        
        Score 0/14
        & 28
        & 24
        & All excluded
        \\ 
        
        Not available
        & 3
        & 3
        & Not included
        \\ 
        \hline
        Total
        & 147
        & 93
        & 12
        \\ 
        \hline
    \end{tabular}    
    \caption{Literature inclusion scores}
    \label{table:literature:inclusionResults}
\end{table}
After excluding results with inclusion score less than 3.5 a total of 28 unique results were review.
Out of the 28 included results 16 were excluded based on exclusion criteria leaving a total of 12 results relevant to \textbf{RQ1} as shown in table \ref{table:literature:results}.
As shown in table \ref{table:literature:resulstByCategory} most of the found literature, $50\%$, compares node.js web server with other programming languages like .net, python and PHP.
One results studied performance impact when monolith application was migrated to microservice.
Other study studied the performance impact of JavaScript API with different JavaScript engines.

\begin{table}[ht!]
    \begin{tabular}{|c c |} 
        \hline
        Literature Id
        & Citation
        \\ 
        \hline\hline
        1
        & \cite{Dafni}
        \\ 
        
        2
        & \cite{Tilkov}
        \\ 
        
        3
        & \cite{Chitra}
        \\ 
        
        4
        & \cite{Shkodra}
        \\ 
        
        5
        & \cite{Chaniotis}
        \\ 
        
        6
        & \cite{NkenyereyeLionel2016PEoN}
        \\ 
        
        7
        & \cite{faustino}
        \\ 
        
        8
        & \cite{Challapalli}
        \\ 
        
        9
        & \cite{Kyriakou}
        \\ 
        
        10
        & \cite{Kyriakou}
        \\ 
        
        11
        & \cite{Lei}
        \\ 
        
        12
        & \cite{SelakovicPerformanceIssues}
        \\ 
        \hline
    \end{tabular}    
    \caption{Literature review results.}
    \label{table:literature:results}
\end{table}


\begin{table}[ht!]
    \begin{tabular}{|c c|} 
        \hline
        Category
        & Literature Id
        \\ 
        \hline\hline
        Node.js I/O performance
        & 1, 10, 11
        \\ 
    
        Description of Node.js as network program
        & 2
        \\ 
    
        Node.js benchmarks
        & 5, 6, 10
        \\ 
    
        Node.js IoT server
        & 6
        \\ 
    
        Migration from monolith to microservice
        & 7
        \\ 
    
        Achieving efficient power usage by scaling CPU frequency
        & 10
        \\ 
    
        Inefficient usage of JavaScript internal APIs effects to performance
        & 12
        \\ 
    
        Comparison of node.js web server with other programming language
        & 3, 4, 5, 8, 9, 11
        \\ 
    
        Comparison against PHP/Apache
        & 5
        \\ 
    
        Comparison against PHP
        & 11
        \\ 
    
        Comparison against Nginx
        & 5
        \\
    
        Comparison against .NET
        & 3, 4
        \\
    
        Comparison against python
        & 8, 11
        \\
    
        Comparison against rust
        & 9
        \\
    
        Comparison against web assembly
        & 9
        \\
        \hline
    \end{tabular}    
    \caption{Literature results by category.}
    \label{table:literature:resulstByCategory}
\end{table}

\section{Sample Data}
Performance samples were collected from the component in both environments sequentially over two month period.
In order to capture the component performance only full cycles were included in the analysis.
Partial cycles and cycles with errors were excluded from the samples since the performance in these cases was irrelevant to the system.
Hardware samples are shown in tables \ref{table:hardware results:monolith:1} and \ref{table:hardware results:independent service:1}.
The response time samples are shown in tables \ref{table:response time results:1}, \ref{table:response time results:2} and \ref{table:response time results:3}.

\subsubsection{Hardware Samples}
Hardware samples were collected along each cycle to make sure that the components response time was not limited by the used hardware.
Only samples from full cycles without errors were included.
The results for monolith are shown in table \ref{table:hardware results:monolith:1} and for independent service in table \ref{table:hardware results:independent service:1}.
In the monolith environment $206965$ hardware samples was collected from $1649$ full cycles.
From the independent service a total of $26902$ hardware samples was collected from $205$ full cycles.
The hardware samples were collected along response time timestamps.
Hardware samples only reflects the systems CPU utilization and memory usage at the point of the timestamp collection and not the status during any meaningful process in the component.

\begin{table}[ht!]
    \begin{tabular}{|c c c c|} 
        \hline
        Monolith
        & CPU (\%)
        & Free Memory (Gigabytes)
        & Total Memory (Gigabytes) \\ [0.5ex] 
        
        \hline\hline
        Mean
        & 25.0
        & 7.588166...
        & 64.265863... \\ 
        
        Median
        & 22.0
        & 3.699269...
        & 64.265863... \\ 

        Minimum
        & 6.0
        & 0.391187...
        & 64.265863... \\ 
        
        Maximum
        & 36.0
        & 55.782023...
        & 64.265863... \\
        \hline
    \end{tabular}
    \caption{Hardware Utilization - Monolith $n=206 965$}
    \label{table:hardware results:monolith:1}
\end{table}

Collected hardware samples shows the systems CPU utilization percentage.
The CPU utilization percentage was calculated as
\[
\textit{Utilization percentage} = 100 \cdot \frac{\textit{Available CPU}}{\textit{Total CPU}}
.\]
Where available CPU is the number of milliseconds the system has spend in the user mode.
The total CPU is the sum of milliseconds the system has spend on user, system and idle modes. The CPU utilization is shown in tables \ref{table:hardware results:monolith:1} and \ref{table:hardware results:independent service:1}.

Collected samples shows that the CPU utilization percentage didn't have any noted effect on the performance since the maximum CPU utilization was at $36.0\%$ in the monolith environment and $1.0\%$ in the independent service.
Leaving most of the CPU available to the application process.
The mean CPU utilization in the monolith is 24 percentage points higher compared to the mean CPU utilization in the independent service.

Collected hardware samples contained information about the free memory and the total available memory for the system.
In table \ref{table:hardware results:monolith:1} the memory data is shown for the monolith application and in table \ref{table:hardware results:independent service:1} for the independent service.
The minimum available memory for the system in monolith was around $0.609...\%$ and in independent service around $61.233...\%$.
Minimum available memory was calculated with the following formula:
\[
\textit{Minimum available memory} = 100 \cdot \frac{\textit{Minimum free memory}}{\textit{Maximum total memory}}
.\]
It is possible that the system would run out of memory.
Out of memory cycles are not shown in samples since the system would raise \textit{out of memory} exception.
The collected memory samples doesn't show out of memory errors since error cases were excluded from the analysis.
The mean available memory was around $11.807...\%$ in monolith and $66.121...\%$ in independent service leaving plenty of memory available for the application use in the collected samples.

\subsubsection{Response Time Samples}
Timestamp samples were collected from both environments to measure the response time of the component.
The results are shown in tables \ref{table:response time results:1}, \ref{table:response time results:2} and \ref{table:response time results:3}.
Start and end timestamps of each cycle were collected along the timestamps before and after all external calls made from the component.

Collected samples contained $1649$ full cycles from monolith environment with mean response time of $\sim198.7$ms.
The mean value was $\sim2.48$ times slower against the minimum value of $\sim 80.2$ms and $\sim5.94$ times faster when compared to the maximum value of $\sim1180.6$ms in the monolith environment.

In the independent service a sample of $205$ full cycles were collected with a mean value of $\sim106.2$ms.
The mean value was $\sim3.14$ times slower compared to the minimum value of $\sim 33.76$ms and $\sim3.80$ times faster compared to the maximum value of $\sim 403.94$ms.

Mean response time in monolith was $\sim1.87$ times slower when compared to the mean response time in the independent service.
The minimum value in the monolith was $\sim 66.63$ percentage points greater that the minimum value in the independent service and the maximum value in the monolith was $\sim 213.57$ percentage points greater when compared to the maximum value in the independent service.

\begin{table}[ht!]
       \begin{tabular}{|c c c c|} 
        \hline
        Independent Service
        & CPU (\%)
        & Free Memory (Gigabytes)
        & Total Memory (Gigabytes) \\ [0.5ex] 
        
        \hline\hline
        Mean
        & 1.0
        & 2.715358... 
        & 4.106678...
        \\
        
        Median
        & 1.0
        & 2.631246...
        & 4.133356...
        \\ 

        Minimum
        & 1.0
        & 2.530976...
        & 4.072448...
        \\ 
        
        Maximum
        & 1.0
        & 3.034747...
        & 4.133356...
        \\
        \hline
    \end{tabular}
    \caption{Hardware Utilization - Independent Service $n=26902$}
    \label{table:hardware results:independent service:1}
\end{table}

\begin{table}[ht!]
    \begin{tabular}{|c c c c|} 
        \hline
        Environment
        & Count (n)
        & Mean (ms)
        & Median (ms)
        \\ [0.5ex] 
        
        \hline\hline
        Monolith
        & 1649
        & 198.747024...
        & 184.522865...
        \\ 
        
        Independent Service
        & 205
        & 106.166740...
        & 88.404872
        \\
        \hline
    \end{tabular}
    \caption{Results for response time samples. Timestamps are in milliseconds.}
    \label{table:response time results:1}
\end{table}

\begin{table}[ht!]
    \begin{tabular}{|c c c|} 
        \hline
        Environment
        & Min (ms)
        & Max (ms) \\ [0.5ex] 
        
        \hline\hline
        Monolith
        & 80.201486... 
        & 1180.642508...
        \\ 
        
        Independent Service
        & 33.763860
        & 403.937152
        \\
        \hline
    \end{tabular}
    \caption{Results for response time samples. Timestamps are in milliseconds.}
    \label{table:response time results:2}
\end{table}

\begin{table}[ht!]
    \begin{tabular}{|c c c|} 
        \hline
        Environment
        & Standard Deviation (ns)
        & Standard Error (ns) \\ [0.5ex] 
        
        \hline\hline
        Monolith
        & $85.409388... \cdot 10^6$
        & $2.103271... \cdot 10^6$
        \\ 
        
        Independent Service
        & $77.391723... \cdot 10^6$
        & $5.405272... \cdot 10^6$
        \\ 
         \hline
    \end{tabular}
    \caption{Results for response time samples. Timestamps are in nanoseconds.}
    \label{table:response time results:3}
\end{table}

\subsubsection{Sample Results}
The component performance was not limited by the hardware according to the collected hardware sample.
Most of the systems CPU was unused and average of at least $11.8\%$ free memory was available for the systems to use.

The response time being $\sim 1.87$ times faster in independent service when compared to monolith environment.
Overall it seems that the components performance in independent service is better when compared to the component performance in monolith.

Collected samples were affected by the runtime condition since the data was collected from production environments.
Collected sample sizes differs by over a thousand sample because only full cycles were included in the analysis.
The data doesn't show how often the component was not running due the required user input for restarts.
